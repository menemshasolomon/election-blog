---
title: 3. Incorporating Polling
author: Mena Solomon
date: '2024-09-18'
slug: []
categories: []
tags: []
---
## How can we best use polls to predict election outcomes?

Polls are central to understanding democratic discourse as they measure the average person's opinion. Since January 1, 2024 there have been **13,626 large-scale presidential polls** conducted to gauge sentiment towards the general election candidates. From 1968 to 2020, *general election polling has largely been an accurate predictor of election outcomes*, with an average polling miss of 1.58 percentage points in terms of popular vote. Much of this accuracy stems from a concept which dates back to Galton's Ox, wherein a large sample of guesses — or, in this case, opinions — will generate an average that approximates the general population's opinion as error in both ways cancels out. [(Galton, 1907)](https://hollis.harvard.edu/permalink/f/1mdq5o5/TN_cdi_crossref_primary_10_1038_075450a0)

Despite the general accuracy of polling right before election day, **polls are often incredibly variable** due to changes in sampling groups, incomplete information, or news events which temporarily alter opinion. The variability of polling is on display in the graphic below, accompanied by the dates of major events which are thought to alter polling answers, at least temporarily. The 1988 election is representative of polling before the digital era, while the 2020 election is the most recent general election. Data on 2024 polling thus far is also included.

```{r  echo = FALSE, message=FALSE, warning=FALSE}
#### Preamble
####-----------------7-----------------------------------------#

# Load libraries.
## install via `install.packages("name")`
library(car)
library(caret)
library(CVXR)
library(glmnet)
library(tidyverse)

## set working directory here
# setwd("~")

####----------------------------------------------------------#
#### Read, merge, and process data.
####----------------------------------------------------------#

# Read data (processed FiveThirtyEight polling average datasets).
d_pollav_natl <- read_csv("data/national_polls_1968-2024.csv")
d_pollav_state <- read_csv("data/state_polls_1968-2024.csv")
```
```{r  echo = FALSE, message=FALSE, warning=FALSE,  fig.width=8, fig.height=12}
####----------------------------------------------------------#
#### Visualizing poll variation over time.
####----------------------------------------------------------#



# Plot 3. Adding in Extra Dates of Interest for 2020 —— "game changers"? 
plot3 <- d_pollav_natl |> 
  filter(year == 2020) |> 
  ggplot(aes(x = poll_date, y = poll_support, color = party)) +
  geom_rect(xmin = as.Date("2020-08-17"), xmax = as.Date("2020-08-20"), ymin = 47.5, ymax = 100, alpha = 0.1, color = NA, fill = "grey") + 
  annotate("text", x = as.Date("2020-08-07"), y = 51.5, label = "DNC", size = 4) +
  geom_rect(xmin = as.Date("2020-08-24"), xmax = as.Date("2020-08-27"), ymin = 0, ymax = 47.2, alpha = 0.1, color = NA, fill = "grey") +
  annotate("text", x = as.Date("2020-09-04"), y = 45, label = "RNC", size = 4) +
  geom_rect(xmin = as.Date("2020-10-02"), xmax = as.Date("2020-10-12"), ymin = 0, ymax = 42.7, alpha = 0.05, color = NA, fill = "grey") +
  
  geom_point(size = 1) + 
  geom_line() + 
  
  geom_segment(x = as.Date("2020-03-12"), xend = as.Date("2020-03-12"), y = 0, yend = 44.8, linetype = "dashed", alpha = 0.4, color = "grey") +
  annotate("text", x = as.Date("2020-03-12"), y = 42.5, label = "COVID \n Market Crash", size = 3) +
  geom_segment(x = as.Date("2020-04-08"), xend = as.Date("2020-04-08"), y = 49, yend = 100, linetype = "dashed", alpha = 0.4, color = "grey") +
  annotate("text", x = as.Date("2020-03-25"), y = 51.3, label = "Bernie Ends Run", size = 3) +
  geom_segment(x = as.Date("2020-04-16"), xend = as.Date("2020-04-16"), y = 0, yend = 44, linetype = "dashed", alpha = 0.4, color = "grey") +
  annotate("text", x = as.Date("2020-04-16"), y = 44.7, label = "22 mil \n Unemployment", size = 3) +
  geom_segment(x = as.Date("2020-05-27"), xend = as.Date("2020-05-27"), y = 0, yend = 43, linetype = "dashed", alpha = 0.4, color = "grey") +
  annotate("text", x = as.Date("2020-06-05"), y = 44, label = "100k COVID Dead, \n George Floyd", size = 3) +
  
  geom_segment(x = as.Date("2020-09-29"), xend = as.Date("2020-09-29"), y = 50, yend = 100, linetype = "dashed", alpha = 0.4, color = "grey") +
  annotate("text", x = as.Date("2020-9-12"), y = 49.5, label = "Pres. Debate", size = 3) +
  geom_segment(x = as.Date("2020-10-07"), xend = as.Date("2020-10-07"), y = 51.7, yend = 100, linetype = "dashed", alpha = 0.4, color = "grey") +
  annotate("text", x = as.Date("2020-10-17"), y = 50.3, label = "VP Debate", size = 3) +
  geom_segment(x = as.Date("2020-10-22"), xend = as.Date("2020-10-22"), y = 52, yend = 100, linetype = "dashed", alpha = 0.4, color = "grey") +
  annotate("text", x = as.Date("2020-10-30"), y = 51.5, label = "Pres. Debate", size = 3) +
  annotate("text", x = as.Date("2020-10-15"), y = 43.7, label = "Trump Has COVID", size = 3) +
  geom_segment(x = as.Date("2020-09-18"), xend = as.Date("2020-09-18"), y = 50, yend = 100, linetype = "dashed", alpha = 0.4, color = "grey") +
  annotate("text", x = as.Date("2020-09-03"), y = 51.5, label = "RBG Passes", size = 3) +
  
  scale_x_date(date_labels = "%b %d") + 
  scale_color_manual(values = c("dodgerblue4", "firebrick1")) +
  labs(x = "Date",
       y = "Average Poll Approval", 
       title = "Polling Averages by Date, 2020 (with Possible Game Changers)") + 
  theme_classic()

# Plot 4. Poll Averages and "Game Changers" for 1988
plot4 <- d_pollav_natl |>
  filter(year == 1988) |>
  ggplot(aes(x = poll_date, y = poll_support, color = party)) +
  geom_rect(xmin=as.Date("1988-07-18"), xmax=as.Date("1988-07-21"), ymin=47, ymax=100, alpha=0.1, colour=NA, fill="grey") +
  annotate("text", x=as.Date("1988-07-10"), y=50, label="DNC", size=4) +
  geom_rect(xmin=as.Date("1988-08-15"), xmax=as.Date("1988-08-18"), ymin=0, ymax=44, alpha=0.1, colour=NA, fill="grey") +
  annotate("text", x=as.Date("1988-08-26"), y=40, label="RNC", size=4) +
  
  geom_point(size = 1) +
  geom_line() + 
  
  geom_segment(x=as.Date("1988-09-13"), xend=as.Date("1988-09-13"), y=49, yend=100, lty=2, color="grey", alpha=0.4) +
  annotate("text", x=as.Date("1988-09-13"), y=52, label="Tank Gaffe", size=3) +
  annotate("text", x=as.Date("1988-09-21"), y=57, label="Willie Horton Ad", size=3) +
  geom_segment(x=as.Date("1988-09-21"), xend=as.Date("1988-09-21"), y=49, yend=100, lty=2, color="grey", alpha=0.4) +
  annotate("text", x=as.Date("1988-10-15"), y=64, label="First Debate\n(Death\nPenalty\nGaffe)", size=3) +
  geom_segment(x=as.Date("1988-10-15"), xend=as.Date("1988-10-15"), y=49, yend=100, lty=2, color="grey", alpha=0.4) +
  scale_x_date(date_labels = "%b, %Y") +
  scale_color_manual(values = c("dodgerblue4","firebrick1")) +
  labs(x = "Date",
       y = "Average Poll Approval", 
       title = "Polling Averages by Date, 1988 (with Possible Game Changers)") + 
  theme_classic()

# Plot 5. Poll Averages for 2024
plot5 <- d_pollav_natl |> 
  filter(year == 2024) |> 
  ggplot(aes(x = poll_date, y = poll_support, color = party)) +
  
  geom_rect(xmin=as.Date("2024-08-19"), xmax=as.Date("2024-08-22"), ymin=45, ymax=100, alpha=0.1, colour=NA, fill="grey") +
  annotate("text", x=as.Date("2024-08-12"), y=48, label="DNC", size=4) +
  geom_rect(xmin=as.Date("2024-07-15"), xmax=as.Date("2024-07-18"), ymin=41, ymax=100, alpha=0.1, colour=NA, fill="grey") +
  annotate("text", x=as.Date("2024-07-26"), y=46, label="RNC", size=4) +
  
  geom_point(size = 1) + 
  geom_line() + 
  
  geom_segment(x=as.Date("2024-06-27"), xend=as.Date("2024-06-27"), y=39, yend=43, lty=2, color="grey", alpha=0.4) +
  annotate("text", x=as.Date("2024-06-27"), y=44, label="First\nPresidential\nDebate", size=3) +
  
  geom_segment(x=as.Date("2024-07-21"), xend=as.Date("2024-07-21"), y=0, yend=43, lty=2, color="grey", alpha=0.4) +
  annotate("text", x=as.Date("2024-07-30"), y=40, label="Biden\nDrops\nOut", size=3) +
  
  
  scale_x_date(date_labels = "%b %d") + 
  scale_color_manual(values = c("dodgerblue4", "firebrick1")) +
  labs(x = "Date",
       y = "Average Poll Approval", 
       title = "Polling Averages by Date, 2024 (with Possible Game Changers)") + 
  theme_classic()

# Load necessary library
library(gridExtra)

# Create a combined plot with grid.arrange
grid.arrange(plot4, plot3, plot5, ncol = 1)
```


## How well do polls predict elections?

**Different pollsters have disparate methods of approaching poll-based predictions.** To better understand various methods of predicting, I will briefly compare the methods used by two preeminent pollsters, Nate Silver, the former director of FiveThirtyEight, and Elliott Morris, FiveThirtyEight's current director. Both Morris and Silver use a combination of weighted polling data as well as economic and political fundamentals to predict the outcome of the election. Silver's model relies heavily on polling data, generating aggregates based on the FiveThiryEight weighting technique while also adjusting to incorporate the likely-voter effect, the house effect, and the timeline effect. Furthermore, Silver weights polling data more heavily than fundamental data — for which he uses only six economic indicators — especially in the weeks leading up to the election [(Silver, 2020)](https://fivethirtyeight.com/features/how-fivethirtyeights-2020-presidential-forecast-works-and-whats-different-because-of-covid-19/). 

After Nate Silver's model failed to accurately predict the 2016 election, Elliott Morris shifted his model's strategy. The Morris model relies heavily on fundamental data, including 11 economic indicators. According to Morris, a baseline fundamental model which includes past elections and economic data should steer the ship by providing an upper and lower bound for estimates which include polling data. Furthermore, Morris utilizes an enhanced understanding of geographic location and demographic information to evaluate and weight polling data [(Morris, 2024)](https://abcnews.go.com/538/538s-2024-presidential-election-forecast-works/story?id=110867585). 

To understand how well polls predict the outcome of general elections, I will use a **multivariate regression which relates aggregate polling data beginning 30 weeks before election day with actual election outcomes.** In doing so, there are a few key assumptions included:

1. **Aggregation** — Aggregate polling data is used here to correct for bias in individual polls. The data used here is from FiveThirtyEight, thus it uses their weighting technique which evaluates the quality of each pollster and then weights based on their rating. For more information, see [FiveThirtyEight's weighting technique](https://fivethirtyeight.com/methodology/how-our-pollster-ratings-work/).

2. **Regularization** — Since this data only includes 11 election cycles (from 1980 to 2024) but 30 variables, there is a risk that the OLS regression model will be over fit toward our data. To correct for this, a regularized regression model will be used which selects for variables which are most significant in predeicting the actual outcome.

3. *Elastic-Net** — The elastic-net regularization technique is a linear combination of Ridge and Lasso penalties. Cross-validation was used to test the optimal lambda value to minimize mean squared error in the regression and, as displayed in the graphic below, elastic-net does the best job of minimizing variance in the model. 


```{r  echo = FALSE, message=FALSE, warning=FALSE}
####----------------------------------------------------------#
#### Regularized regression with polling data.
####----------------------------------------------------------#

# Read election results data. 
d_vote <- read_csv("data/popvote_1948-2020.csv")
d_vote$party[d_vote$party == "democrat"] <- "DEM"
d_vote$party[d_vote$party == "republican"] <- "REP"

# Create dataset of polling average by week until the election. 
d_poll_weeks <- d_pollav_natl |> 
  group_by(year, party, weeks_left) |>
  summarize(mean_poll_week = mean(poll_support)) |> 
  filter(weeks_left <= 30) |> 
  pivot_wider(names_from = weeks_left, values_from = mean_poll_week) |> 
  left_join(d_vote, by = c("year", "party"))
 
# Split into training and testing data based on inclusion or exclusion of 2024. 
d_poll_weeks_train <- d_poll_weeks |> 
  filter(year <= 2020)
d_poll_weeks_test <- d_poll_weeks |> 
  filter(year == 2024)

colnames(d_poll_weeks)[3:33] <- paste0("poll_weeks_left_", 0:30)
colnames(d_poll_weeks_train)[3:33] <- paste0("poll_weeks_left_", 0:30)
colnames(d_poll_weeks_test)[3:33] <- paste0("poll_weeks_left_", 0:30)

# Comparison of OLS and regularized regression methods. 
ols.pollweeks <- lm(paste0("pv2p ~ ", paste0( "poll_weeks_left_", 0:30, collapse = " + ")), 
                    data = d_poll_weeks_train)

# Separate data into X and Y for training. 
x.train <- d_poll_weeks_train |>
  ungroup() |> 
  select(all_of(starts_with("poll_weeks_left_"))) |> 
  as.matrix()
y.train <- d_poll_weeks_train$pv2p


# Ridge. --> find formula for ridge in the slides (looks like lasso from gov51)
# larger penalty term yields smaller coefficients but never zero 
ridge.pollsweeks <- glmnet(x = x.train, y = y.train, alpha = 0) # Set ridge using alpha = 0. 

# Lasso. --> same as ridge but with the L1 nrom instead of the L2 norm (absolute value beta coef instead of squared)
# Lasso will remove coefficients from prediction model, often used for feature selection
lasso.pollsweeks <- glmnet(x = x.train, y = y.train, alpha = 1) # Set lasso using alpha = 1.

# Elastic net. --> combines ridge and lasso, allows for zeros but squares terms instead of abs value
enet.pollsweeks <- glmnet(x = x.train, y = y.train, alpha = 0.5) # Set elastic net using alpha = 0.5. (equally weighted from ridge and lasso)


# Can use cross-validated versions to find the optimal values of lambda that minimize the MSE of your predictions. 
# N.B. Use set.seed() and your favorite number e.g., 12345, 02138, before each CV/any stochastic call if you want your results to be stable. 
cv.ridge.pollweeks <- cv.glmnet(x = x.train, y = y.train, alpha = 0)
cv.lasso.pollweeks <- cv.glmnet(x = x.train, y = y.train, alpha = 1)
cv.enet.pollweeks <- cv.glmnet(x = x.train, y = y.train, alpha = 0.5)

# Get minimum lambda values 
lambda.min.ridge <- cv.ridge.pollweeks$lambda.min
lambda.min.lasso <- cv.lasso.pollweeks$lambda.min
lambda.min.enet <- cv.enet.pollweeks$lambda.min

# Predict on training data using lambda values that minimize MSE. --> Lasso minimizes MSE
mse.ridge <- mean((predict(ridge.pollsweeks, s = lambda.min.ridge, newx = x.train) - y.train)^2)
mse.lasso <- mean((predict(lasso.pollsweeks, s = lambda.min.lasso, newx = x.train) - y.train)^2)
mse.enet <- mean((predict(enet.pollsweeks, s = lambda.min.enet, newx = x.train) - y.train)^2)

# Generate plot comparing coefficients for each of the weeks. --> include all poll data to train a model, what would the size of coefficients be?
# - these coefficients cannot be interpreted because we have biased our model to decrease variance
# Create the data frame and pivot it
d.coefplot <- data.frame(
  "OLS" = coef(ols.pollweeks)[-1], 
  "Ridge" = coef(ridge.pollsweeks, s = lambda.min.ridge)[-1], 
  "Lasso" = coef(lasso.pollsweeks, s = lambda.min.lasso)[-1], 
  "Elastic-Net" = coef(enet.pollsweeks, s = lambda.min.enet)[-1]
) |>
  rownames_to_column("coef_name") |> 
  pivot_longer(cols = -coef_name, names_to = "Method", values_to = "coef_est") |> 
  mutate(week = rep(0:30, each = 4))

# Replace NA values in coef_est with 0
d.coefplot[which(is.na(d.coefplot$coef_est)),]$coef_est <- 0 

# Rename the coefficients to "Week 0", "Week 1", etc.
d.coefplot <- d.coefplot |>
  mutate(coef_name = paste0(week))

# Plot the graph
d.coefplot |>
  ggplot(aes(x = coef_est, y = reorder(coef_name, -week), color = Method)) +
  geom_segment(aes(xend = 0, yend = reorder(coef_name, -week)), alpha = 0.5, lty = "dashed") +
  geom_vline(aes(xintercept = 0), lty = "dashed") +   
  geom_point() + 
  labs(x = "Coefficient Estimate", 
       y = "Week Variable", 
       title = "Comparison of Coefficients Across Regularization Methods") + 
  theme_classic()

```


According to the Elastic-Net model, the only statistically significant predictors are *weeks 0, 2, 5, 9, 11, 12, 15, 18, 21, 24, 25, 26, and 30*. These coefficiants are not cannot be interpreted becaused they are biased for the sake of decreasing model variance. 

While it is difficult to determine exactly why the weeks listed above have more predictive power than other weeks, my intuition is that the earlier weeks are significant due to the power of fundamentals and partisan stickiness. Indeed, most individuals are loyal to a particular party and are able to determine their vote based on a few economic [fundamentals](https://menemshasolomon.github.io/election-blog/post/2024-09-14-2-the-importance-of-the-economy/) well before election day. In the lead up to the election, however, the news cycle is dominated by general election coverage and information, generating two effects. First, right before election day the polls tend to converge as voters become enlightened, making later polls more accurate. Second, poll volatility is often a reflection of media volatility within a media-driven election environment, thus in the weeks before elections small moments have large effects on polling, decreasing the reliability of this data[(Gelman and King, 1993)](https://hollis.harvard.edu/permalink/f/1mdq5o5/TN_cdi_webofscience_primary_A1993MC65200001CitationCount).

## Using these models, how can polling be used to predict the outcome of the 2024 election?

```{r   echo = FALSE, message=FALSE, warning=FALSE}
# First check how many weeks of polling we have for 2024. 
weeksleftdata <- d_pollav_natl |> 
  filter(year == 2024) |> 
  select(weeks_left) |> 
  distinct() |> 
  range() # Let's take week 30 - 7 as predictors since those are the weeks we have polling data for 2024 and historically. 

x.train2 <- d_poll_weeks_train |>
  ungroup() |> 
  select(all_of(paste0("poll_weeks_left_", 7:30))) |> 
  as.matrix()
y.train2 <- d_poll_weeks_train$pv2p
x.test2 <- d_poll_weeks_test |>
  ungroup() |> 
  select(all_of(paste0("poll_weeks_left_", 7:30))) |> 
  as.matrix()

# Using elastic-net for simplicity. 
set.seed(02138)
enet.poll <- cv.glmnet(x = x.train2, y = y.train2, alpha = 0.5)
lambda.min.enet.poll <- enet.poll$lambda.min

# Predict 2024 national pv2p share using elastic-net
polls.pred <- predict(enet.poll, s = lambda.min.enet.poll, newx = x.test2)

#Normalize the predictions
polls.pred.normalized <- polls.pred / sum(polls.pred) * 100

```


```{r   echo = FALSE, message=FALSE, warning=FALSE}
# Estimate models using polls alone, fundamentals alone, and combined fundamentals and polls. 
# this is one option for a weighting scheme
# Read economic data. 
d_econ <- read_csv("data/fred_econ.csv") |> 
  filter(quarter == 2)

# Combine datasets and create vote lags. 
d_combined <- d_econ |> 
  left_join(d_poll_weeks, by = "year") |> 
  filter(year %in% c(unique(d_vote$year), 2024)) |> 
  group_by(party) |> 
  mutate(pv2p_lag1 = lag(pv2p, 1), 
         pv2p_lag2 = lag(pv2p, 2)) |> 
  ungroup() |> 
  mutate(gdp_growth_x_incumbent = GDP_growth_quarterly * incumbent, 
         rdpi_growth_quarterly = RDPI_growth_quarterly * incumbent,
         cpi_x_incumbent = CPI * incumbent,
         unemployment_x_incumbent = unemployment * incumbent,
         sp500_x_incumbent = sp500_close * incumbent) # Generate interaction effects.

# Create fundamentals-only dataset and split into training and test sets. 
d_fund <- d_combined |> 
  select("year", "pv2p", "GDP", "GDP_growth_quarterly", "RDPI", "RDPI_growth_quarterly", "CPI", "unemployment", "sp500_close",
         "incumbent", "gdp_growth_x_incumbent", "rdpi_growth_quarterly", "cpi_x_incumbent", "unemployment_x_incumbent", "sp500_x_incumbent", 
         "pv2p_lag1", "pv2p_lag2") 
x.train.fund <- d_fund |> 
  filter(year <= 2020) |>
  select(-c(year, pv2p)) |> 
  slice(-c(1:9)) |> 
  as.matrix()
y.train.fund <- d_fund |> 
  filter(year <= 2020) |> 
  select(pv2p) |> 
  slice(-c(1:9)) |> 
  as.matrix()
x.test.fund <- d_fund |> 
  filter(year == 2024) |> 
  select(-c(year, pv2p)) |> 
  drop_na() |> 
  as.matrix()

# Estimate elastic-net using fundamental variables only.
set.seed(02138)
enet.fund <- cv.glmnet(x = x.train.fund, y = y.train.fund, intercept = FALSE, alpha = 0.5)
lambda.min.enet.fund <- enet.fund$lambda.min

# Predict 2024 national pv2p share using elastic-net. 
fund.pred <- predict(enet.fund, s = lambda.min.enet.fund, newx = x.test.fund)
  
# Sequester data for combined model.
d_combo <- d_combined |> 
  select("year", "pv2p", "GDP", "GDP_growth_quarterly", "RDPI", "RDPI_growth_quarterly", "CPI", "unemployment", "sp500_close",
         "incumbent", "gdp_growth_x_incumbent", "rdpi_growth_quarterly", "cpi_x_incumbent", "unemployment_x_incumbent", "sp500_x_incumbent", 
         "pv2p_lag1", "pv2p_lag2", all_of(paste0("poll_weeks_left_", 7:30))) 

x.train.combined <- d_combo |> 
  filter(year <= 2020) |> 
  select(-c(year, pv2p)) |> 
  slice(-c(1:9)) |> 
  as.matrix()
y.train.combined <- d_combo |>
  filter(year <= 2020) |> 
  select(pv2p) |> 
  slice(-c(1:9)) |> 
  as.matrix()
x.test.combined <- d_combo |>
  filter(year == 2024) |> 
  select(-c(year, pv2p)) |> 
  drop_na() |> 
  as.matrix()
  
# Estimate combined model.
set.seed(02138)
enet.combined <- cv.glmnet(x = x.train.combined, y = y.train.combined, intercept = FALSE, alpha = 0.5)
lambda.min.enet.combined <- enet.combined$lambda.min

# Predict 2024 national pv2p share using elastic-net.
combo.pred <- predict(enet.combined, s = lambda.min.enet.combined, newx = x.test.combined)

#table
# Candidate names for 2024 (example)
candidates <- c("Kamala Harris", "Donald Trump")

predictions_df <- data.frame(
  Candidate = candidates,
  Polling_Model = polls.pred.normalized,
  Fundamentals_Model = fund.pred,
  Combined_Model = combo.pred
)

colnames(predictions_df) <- c("Candidate", 
                                "Polling Model", 
                                "Fundamentals Model", 
                                "Combined Model")

# Print
library(knitr)
kable(predictions_df)


```

The elastic-net regularized regression model trained above uses aggregate polling data to predict the national two-party vote share in 2024. **Using polling as a predictor, the normalized model estimates a 50.55% two-party popular vote share for Vice President Harris and a 49.45% two-party popular vote share for former President Trump, giving Harris the majority.**

Furthermore, using an elastic-net regularized regression of the [economic fundamental model from last week](https://menemshasolomon.github.io/election-blog/post/2024-09-14-2-the-importance-of-the-economy/) including the effect of GDP growth, RDPI growth, CPI, and unemployment on incumbent vote share then allows for the creation of a *combined model*. **This combined model, which incorporates both economic fundamentals and polling data, estimates a 50.48% two-party popular vote share for Vice President Harris and a 47.94% two-party popular vote share for former President Trump, once again giving Harris the majority.**

## Notes
All code above is accessible via [Github](https://github.com/menemshasolomon/election-blog/blob/main/content/post/2024-09-18-3-incorporating-polling/index.Rmarkdown).

**Sources**

Galton, Francis. "Vox Populi." Nature 75, no. 1949 (1907): 450-51. https://doi.org/10.1038/075450a0. 
 
Gelman, Andrew, and Gary King. "Why Are American Presidential Election Campaign Polls so Variable 
     When Votes Are so Predictable?" British Journal of Political Science 23, no. 4 (1993): 409-51. 
     https://doi.org/10.1017/s0007123400006682. 

"How Our Pollster Ratings Work." FiveThirtyEight. Last modified March 10, 2023. Accessed September 
     22, 2024. https://fivethirtyeight.com/methodology/how-our-pollster-ratings-work/. 

Morris, G. Elliott. "How 538's 2024 presidential election forecast works." Abc News. Last modified 
     June 11, 2024. Accessed September 23, 2024. https://abcnews.go.com/538/ 
     538s-2024-presidential-election-forecast-works/story?id=110867585.      

Silver, Nate. "How FiveThirtyEight's 2020 Presidential Forecast Works — And What's Different 
     Because Of COVID-19." FiveThirtyEight. Last modified August 12, 2020. Accessed September 23, 
     2024. https://fivethirtyeight.com/features/ 
     how-fivethirtyeights-2020-presidential-forecast-works-and-whats-different-because-of-covid-19/. 
     
**Data Source**

US Presidential Election Popular Vote Data from 1948-2020 provided by the course. Economic data from the U.S. Bureau of Economic Analysis, also provided by the course. Polling data sourced from FiveThirtyEight, also provided by the course.

